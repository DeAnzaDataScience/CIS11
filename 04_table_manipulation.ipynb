{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bie3tg8sOuGm"
   },
   "source": [
    "# Table Manipulation\n",
    "\n",
    "## Reading\n",
    "\n",
    "[Chapter 8: 8.2 - 8.4](https://inferentialthinking.com/chapters/08/Functions_and_Tables.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUmOtP4zPJSK"
   },
   "source": [
    "\n",
    "In this notebook we explore more advanced methods to work with data in a table or DataFrame.\n",
    "\n",
    "First we'll learn to write functions that can help us work more efficiently, then we learn how to build and reshape a DataFrame, and how to separate data into groups to evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFQSnJVSRNLF"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZRefuDgcl-A"
   },
   "source": [
    "## Writing Functions\n",
    "Functions help us work more efficiently with the code that we use for data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLM611T_daKC"
   },
   "source": [
    "As shown in the textbook, suppose we have a dataset of the heights of parents and children in different families. We want to use the dataset to predict a child's height, given the parents' heights.\n",
    "\n",
    "First, we read in the height data into a table or DataFrame, and then we inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoVZilTTOVfT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/galton.csv\"\n",
    "family = pd.read_csv(url)\n",
    "print(\"First 5 rows:\")\n",
    "family.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTJc_k-CTcwX"
   },
   "source": [
    "Each row is for one child in the family, and the columns that we're interested in are the heights of the `father`, the `mother`, and the `childHeight`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQMQZQStT3ZO"
   },
   "source": [
    "Next, we would like to predict the child's height based on the parents' average heights.\n",
    "\n",
    "<u>Step 1</u>:<br>\n",
    "We select the parents' height columns, find the average of the parents' height, and create a DataFrame of the heights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7wLj_iMhXFfi"
   },
   "outputs": [],
   "source": [
    "# find the average of the parents' height\n",
    "avg = (family.father + family.mother) / 2\n",
    "\n",
    "# create a new DataFrame with the avg and childHeight data\n",
    "heights = pd.DataFrame({'avg': avg,\n",
    "                        'child': family.childHeight})\n",
    "print(\"First 5 rows:\")\n",
    "heights.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0Y_q296ZldV"
   },
   "source": [
    "<u>Step 2</u>:<br>\n",
    "We use a scatterplot to see if there's correlation between the parents' average height and the child's height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0RaWPFvZtzC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(heights.avg, heights.child, alpha=0.5)\n",
    "plt.title(\"Child Height vs. Parent Average Height\")\n",
    "plt.xlabel(\"Parent Average Height\")\n",
    "plt.ylabel('Child Height')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O27O5MUnadcz"
   },
   "source": [
    "It looks like there's positive correlation: in general, the taller the parents, the taller the child. Because there is correlation, we can use the parents' data to predict the child's height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_Rqs5G4asyn"
   },
   "source": [
    "Suppose we want to predict the child's height, given that the parents' average height is 68 inches.\n",
    "\n",
    "<u>Step 3</u>:<br>\n",
    "From the dataset, we:\n",
    "1. Extract all the average parents' heights that are within 1/2 inch from 68 inches, which is the range of 67.5 to 68.5 inches.\n",
    "2. Use these parent height averages to look up the corresponding child's heights, and find the mean of all the child heights.\n",
    "\n",
    "The resulting mean will be our predicted child height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPEbqiSZsL8F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Find all the parent height averages in the range around 68 inches\n",
    "# and create a new DataFrame with them\n",
    "parent_range = heights[(heights.avg >= (68-0.5)) & (heights.avg < (68+0.5))]\n",
    "\n",
    "# 2. Find the mean of all the corresponding child heights\n",
    "child_height = np.mean(parent_range.child)\n",
    "\n",
    "# 3. Print the child's height\n",
    "print(\"Predicted child's height:\", round(child_height, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVPavKbme84z"
   },
   "source": [
    "The method of averaging a range of data that are close to our target data (68 inches) and use the average to predict the child's height is called the _nearest neighbor_ prediction. This method is the foundation in many machine learning algorithms that are used for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Paua2-Dyu9MB"
   },
   "source": [
    "Now we want to make the above prediction calculation for _any_ parent average height. This means we want to find a way to conveniently run the lines of code above, without having to retype them for each prediction. The convenient way is to put the 3 lines of code into a _function_.\n",
    "\n",
    "<u>Step 4</u>:<br>\n",
    "Generalize the prediction method by _writing a function_ to use with any parent average height.\n",
    "\n",
    "Recall that we first worked with functions in Week 2, where we used or _called_ functions such as round() or print(). A function is a block of code that has a name and performs a task. The print() function has the name _print_ and its job is to print all the data that are put in between the parentheses `( )`.\n",
    "\n",
    "We now learn to write a function or the block of code so that will do the work that we want. Then we call our function in the same way that we call print().\n",
    "\n",
    "To write our own function we use the following format:\n",
    "\n",
    "```\n",
    "def function_name(input to function):\n",
    "    steps to do a task\n",
    "    return result from the task\n",
    "```\n",
    "\n",
    "- The keyword `def` is short for _definition_. A function definition is the block of code that does the task of the function.\n",
    "- The function_name can be any word(s) that describes what task the function does.\n",
    "- The `(input to function)` are the variables that store the input to the function.\n",
    "- The first line of the function definition ends with colon `:`. This line is called the _function header_.\n",
    "- The block of code that performs the function's task is called the _function body_. It is indented from the function header.\n",
    "- If the function's task produces new data or result, the result is `return`ed as output from the function.\n",
    "\n",
    "In the Code cell above, we notice that our task to predict a child's height is made up of 3 steps. Therefore, putting together the function definition format and the 3 lines of code that make up the function's task, we have:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2U1BE-0wabO"
   },
   "outputs": [],
   "source": [
    "# The function name is predict_child\n",
    "# The input is the parent height average\n",
    "def predict_child(parent_avg):\n",
    "\n",
    "  # 1. find the parent height averages within the 1/2 inch range\n",
    "  parent_range = heights[(heights.avg >= (parent_avg-0.5)) &\n",
    "                         (heights.avg < (parent_avg+0.5))]\n",
    "  # 2. find the child's height\n",
    "  child_avg = np.mean(parent_range.child)\n",
    "  # 3. return the child's height\n",
    "  return np.round(child_avg, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKOoB3x-cnpI"
   },
   "source": [
    "Comparing the 2 lines of code where we calculated the predicted child height from the parent average of 68:\n",
    "```\n",
    "parent_range = heights[(heights.avg >= (68-0.5)) & (heights.avg < (68+0.5))]\n",
    "child_height = np.mean(parent_range.child)\n",
    "print(\"Child's height:\", round(child_height, 2))\n",
    "```\n",
    "We notice that there are 2 changes that we made in the original code:\n",
    "- substitute the value `68` with the variable `parent_avg`\n",
    "- instead of printing the child's height, we `return` the child's height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNrWpViIxycU"
   },
   "source": [
    "To run the function with a parent height average as input, we use the format:<br>\n",
    "`returned_data = function_name(input data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiAnbcbTx4FM"
   },
   "outputs": [],
   "source": [
    "child_height = predict_child(68)\n",
    "print(\"Parent:\", 68, \"Child:\", child_height)\n",
    "\n",
    "child_height = predict_child(66)\n",
    "print(\"Parent:\", 66, \"Child:\",child_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Scv9QIRKi6I2"
   },
   "source": [
    "Note that after the `predict_child` function is written, we can run or call it any time we want to predict a child's height, we don't need to repeat the 3 calculation steps each time.\n",
    "\n",
    "The advantage of writing _functions_ are:\n",
    "- Avoid repetition: we don't need to copy and paste a block of code over and over each time we want to do the same task.\n",
    "- Easier to read the code: it's easier to recognize what `predict_child` would do, compared to reading the 3 lines of code that make up this function.\n",
    "- Easier to maintain the code: if we decided to change the calculation to predict the child's age, we would only need to make the change in the `predict_child` function, instead of searching for the 3 lines of code and change them throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLefbaLHnPYm"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80I3Sk3KnS-P"
   },
   "source": [
    "## Grouping Data by a Feature\n",
    "\n",
    "Often data scientists need to divide the dataset into groups to inspect the data in each group or to compare the groups.\n",
    "\n",
    "For example, if the dataset contains rows of information about the ice cream that people buy at a store:\n",
    "- what time of year: spring, summer, fall, winter\n",
    "- the amount of ice cream: how many scoops\n",
    "- the ice cream flavor: chocolate, strawberry, vanilla\n",
    "- the price: how much per scoop\n",
    "\n",
    "If we want to find the data patterns for a specific flavor of ice cream, we can save all the rows of data for chocolate into a group, and all the rows for strawberry and for vanilla into two other groups. With these groups, we can look for the popular time of year for chocolate vs. strawberry vs. vanilla, or what's the total amount of money people spend on a specific ice cream flavor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ2I_WeFs9sg"
   },
   "source": [
    "To form groups based on a data feature, such as the ice cream flavor, the feature must be _categorical data_.\n",
    "\n",
    "Recall that categorical data means that the data must be its own distinct category and not part of a continuous range. In the ice cream example, the time of year (the 4 seasons) and the ice cream flavor (3 flavors) are categorical data. They are not part of a continuous range of data.\n",
    "\n",
    "Can you tell which feature of the ice cream dataset is not categorical data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjaejoOnqoco"
   },
   "source": [
    "We now use a dataset to explore how to form groups of data by a feature. This dataset is used in the book and is a dataset of NBA player salaries, which we read in into a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZBurlbCi5un"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/nba-salaries.csv'\n",
    "nba = pd.read_csv(url)\n",
    "nba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcLQGXQy1iKs"
   },
   "source": [
    "We want to look at the latest season only, so we first create a DataFrame where the season is 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDD60eGC142a"
   },
   "outputs": [],
   "source": [
    "latest_nba = nba[nba.season == 2020]\n",
    "latest_nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WVbF3Djhsdit"
   },
   "source": [
    "In the dataset above, the `position` and `team` are the categorical data.\n",
    "\n",
    "To form groups based on a feature, pandas provides us with a `groupby` method and several options such as: `count`, `sum`, `max`, `min`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xzud8XCVvq_v"
   },
   "source": [
    "### Count of groups\n",
    "To group the data by a feature and find the total count of each category of feature, we use the format:<br>\n",
    "`output = a_DataFrame.groupby('feature').a_column.count()`\n",
    "\n",
    "The groupby method returns each group by `feature`, complete with all the columns except the feature column, so we need to choose a colum and then we ask for the count of that column. We can choose any column since the count is the same for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb-swonuu4z7"
   },
   "outputs": [],
   "source": [
    "# group by position and find the count of each position\n",
    "latest_nba.groupby('position').name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BB5ImzO8vqE1"
   },
   "outputs": [],
   "source": [
    "# Can you write code to print each team name and the count of each team?\n",
    "# Hint: group the data by team name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6MGZkr-xuHC"
   },
   "source": [
    "### Basic Statistics of Groups\n",
    "To group the data by a feature and find the statistics of a certain column in each group, we use the format:<br>\n",
    "`output = a_DataFrame.groupby('feature').a_column.statistical_function()`\n",
    "\n",
    "The groupby method returns each group by feature, complete with all the columns except the feature column, so we need to choose the column that we want and ask for the specific statistics, some of which are:\n",
    "- sum()\n",
    "- max()\n",
    "- min()\n",
    "- mean()\n",
    "- median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PglF8Gfsv7P_"
   },
   "outputs": [],
   "source": [
    "# find the mean salary of each position\n",
    "avg_salary = latest_nba.groupby('position').salary.mean()\n",
    "print(\"Salary in millions\")\n",
    "round(avg_salary/1000000,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MGs25DY0zTH"
   },
   "outputs": [],
   "source": [
    "# Can you find how much each team pays all of its players?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0D8mFpDU3W-P"
   },
   "source": [
    "### Grouping by Two Features\n",
    "To group data by more than one features, we simply give the `groupby`method the features we want. We do this by listing the features inside `[]`, which you may recall is the format of a list in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sXmQQZC3fJt"
   },
   "outputs": [],
   "source": [
    "# show each position, and with each position show all the teams\n",
    "# along with the count of that position in the team\n",
    "latest_nba.groupby(['position', 'team']).name.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUy6Stcg5cIh"
   },
   "source": [
    "Separating data into groups is useful when we need to compare the groups or do calculations on groups of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFjlmLDL5Zqe"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2pPKpiW5dU2"
   },
   "source": [
    "## Pivoting\n",
    "Sometimes it is useful to reshape the data in a table in order to see the patterns in the data more clearly. When we select certain columns (or features of the data) and display them against each other, this is called _pivoting_ data.\n",
    "\n",
    "To pivot, we use 3 columns of the DataFrame as the key components of a pivot:\n",
    "- One column will stay the same, with rows of data, and serves as the _index_ of the pivot.\n",
    "The data must be categorical data.\n",
    "- One column will be separated into multiple columns and serves as the _columns_ of the pivot. The separated column labels are the unique categories of data, and the data must be categorical data.\n",
    "- One column will be used to fill in the new  table, and serve as the _values_ of the pivot.\n",
    "\n",
    "Optionally we can also specify the _aggfunc_, which tells the pivot method to do some basic calculation with the _values_ data. The basic calculation can be finding the sum, the mean, the count, the max, or the min of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjQJeGTsY7x6"
   },
   "source": [
    "Suppose we have a dataset of the 2 types of drinks and their popularity ranking during 3 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okbsqUTTWyK6"
   },
   "outputs": [],
   "source": [
    "drinks = pd.DataFrame({'drink': ['coffee', 'tea', 'coffee', 'tea', 'coffee', 'tea'],\n",
    "                      'year': [2023, 2023, 2022, 2022, 2021, 2021],\n",
    "                      'ranking': [1, 2, 2, 1, 1, 2]})\n",
    "drinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfS2uw9PZzen"
   },
   "source": [
    "With the table above it's not easy to see which drink is more popular during the 3 years. To make it easier to see the data, we pivot the table by keeping the same `drink` column, but changing the `year` column into separate columns. This will make it easier to see the ranking.\n",
    "\n",
    "We use the `pivot` method and we need to give 3 input to the method:\n",
    "- The `index` input will be the `drink` column.\n",
    "- The `columns` input will be the `year` column.\n",
    "- The `values` input will be the `rank` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXbLoBngYDy8"
   },
   "outputs": [],
   "source": [
    "drinks.pivot(index='drink', columns='year', values='ranking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6q1UpJPbnRF"
   },
   "source": [
    "Now it's easier to see that coffee is more popular than tea since it's ranked first in 2 of the 3 years.\n",
    "\n",
    "We can also pivot the other way by swapping the `drink` and `year` in the new table. This new table will also show more clearly that coffee is the popular drink than the original table could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tS_Z_Cb5_LVx"
   },
   "outputs": [],
   "source": [
    "drinks.pivot(index='year', columns='drink', values='ranking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-6PRqQZb0T8"
   },
   "source": [
    "Next we look at a larger dataset which is introduced in the textbook. The dataset is about personal income and education level for the US population from 2008 to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IdEIN2qn3ojh"
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/DeAnzaDataScience/CIS11/refs/heads/main/datasets_notes/educ_inc.csv'\n",
    "salary = pd.read_csv(url)\n",
    "salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2lMdCQviLNv"
   },
   "source": [
    "We're interested in the `Educational Attainment`, `Personal Income`, and `Population Count` columns, so we create a new DataFrame with these 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G6j-SJyujKAX"
   },
   "outputs": [],
   "source": [
    "salary_education = pd.DataFrame({'Education': salary['Educational Attainment'], \n",
    "                                 'Income': salary['Personal Income'], \n",
    "                                 'Population' : salary['Population Count']})\n",
    "salary_education.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_2dPv3XL09P"
   },
   "source": [
    "Since we will look at the total population count between 2008 and 2014, which is a large number, we will scale down the population so it's in units of 1 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUM5oH5FC9mm"
   },
   "outputs": [],
   "source": [
    "salary_education.Population = salary_education.Population / 1000000\n",
    "salary_education.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWAkj1CX3yCM"
   },
   "source": [
    "From the table it's not easy to see the income vs education level, so we pivot the data by showing a table with rows of `Income` and columns of `Educational`. In addition, we want to sum up the total number of people for each category, so we give the `aggfunc` input the `sum` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kccz97-lkM2g"
   },
   "outputs": [],
   "source": [
    "totals = salary_education.pivot_table(index='Income',\n",
    "                               columns='Education',\n",
    "                               values='Population',\n",
    "                               aggfunc='sum')\n",
    "totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gq5x0oogAYE9"
   },
   "source": [
    "We now see that for the lowest income, level A, the largest population has no high school diploma. And for the highest income, level H, the largest population has a Bachelor's degree or higher.\n",
    "\n",
    "We can also put our data visualization skills to work and plot the population count for the lowest and highest income level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDYXKf9vJQ6_"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.barh(totals.index, totals.iloc[:,0])\n",
    "plt.title(\"Bachelor's Degree or Higher\")\n",
    "plt.xlabel(\"Population (millions)\")\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRchUR5jJgVH"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.barh(totals.index, totals.iloc[:,3])\n",
    "plt.title(\"No High School Diploma\")\n",
    "plt.xlabel(\"Population (millions)\")\n",
    "plt.grid()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LVQLoBONfg7"
   },
   "source": [
    "The plots show the income advantage of a college degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U755_BB7X4w_"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jk0MTTTnX7HE"
   },
   "source": [
    "## Joining\n",
    "Sometimes the data we need are stored in different tables. Each table shares some user-related data, but each table also stores some unique user data.\n",
    "\n",
    "An example would be when a traveler books a trip by using a travel aggregator (like Expedia) to buy their airline ticket and hotel.\n",
    "- The travel aggregator table will store the traveler's ID, name, email, booking date, flight id, hotel id, and payment.\n",
    "- The airline table will store the traveler's airline ID, name, email, flight number, departure airport, arrival airport, baggage status.\n",
    "- The hotel table will store the traveler's hotel ID, name, email, room type, date and length of stay.\n",
    "\n",
    "The data for the trip are across 3 different tab;es, but they share the traveler's name and email.\n",
    "\n",
    "In this case we need to combine or _join_ the 3 tables into one table before we can analyze the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrYw_yUHb1GQ"
   },
   "source": [
    "To see how joining 2 tables work, we use the same `drinks` dataset of coffee and tea that we used in the pivot section above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01Q9iFpXZzWT"
   },
   "outputs": [],
   "source": [
    "drinks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyyC-RGhdZQx"
   },
   "source": [
    "Suppose we have another table that shows the price of the coffee and tea as well as an extra drink, soda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tND5aOwuZ7wP"
   },
   "outputs": [],
   "source": [
    "prices = pd.DataFrame({'drink': ['coffee', 'tea','soda'],\n",
    "                       'price': [3.5, 2.5, 1.0]})\n",
    "prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aH9Gafddrlb"
   },
   "source": [
    "We can see that the `drink` column is where the 2 tables have similar data.\n",
    "\n",
    "We join the `drinks` and `prices` table by using the format:\n",
    "`table1.join(table2.set_index(common_column), on = common_column)`\n",
    "\n",
    "With this format we add table2 into table1. Both tables need to specify the common column between the 2 tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fycxsJmaC30"
   },
   "outputs": [],
   "source": [
    "drinks.join(prices.set_index('drink'), on ='drink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GAoOep5nKhs"
   },
   "source": [
    "Now we have a table with data from both `drinks` and `prices`. Note that:\n",
    "- Even though `coffee` appears one time in `prices`, it has been copied into each of the `coffee` rows in `drinks`.\n",
    "- The `soda` row doesn't appear in the final table since it's not in the `drinks` table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWACPiY4N_Sj"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vWI6FSNOBcw"
   },
   "source": [
    "In this notebook we learn how to combine code together into functions, and how to combine data together in groups, pivot table, and new DataFrames. We now have enough experience with programming tools, and in the next notebooks we'll apply them in statistical methods that help us analyze data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNOUbngaZCKJeD//1GT2yV2",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
